{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from scipy.stats import bootstrap\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from pingouin import wilcoxon\n",
    "\n",
    "\n",
    "def read_res(\n",
    "    data,\n",
    "    model,\n",
    "    trainer,\n",
    "    data_path,\n",
    "    base_path=Path(\"/data/home/meiri.yoav/Cognitive-State-Decoding\"),\n",
    "    base_res_path=\"cross_validation_runs\",\n",
    "    wandb_job_type=\"cv\",\n",
    "    template=\"{}/+data={},+data_path={},+model={},+trainer={},trainer.wandb_job_type={}\",\n",
    "    on_error=\"raise\",\n",
    ") -> pd.DataFrame | None:\n",
    "    file_path = (\n",
    "        base_path\n",
    "        / template.format(\n",
    "            base_res_path, data, data_path, model, trainer, wandb_job_type\n",
    "        )\n",
    "        / \"trial_level_test_results.csv\"\n",
    "    )\n",
    "    try:\n",
    "        res = pd.read_csv(\n",
    "            file_path,\n",
    "            # index_col=0,\n",
    "            usecols=[\n",
    "                \"eval_regime\",\n",
    "                \"eval_type\",\n",
    "                # \"subjects\",\n",
    "                # \"items\",\n",
    "                \"binary_prediction\",\n",
    "                \"binary_label\",\n",
    "                \"prediction_prob\",\n",
    "                \"fold_index\",\n",
    "            ],\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        if on_error == \"raise\":\n",
    "            raise e\n",
    "        else:\n",
    "            return None\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy_diff(y_true1, y_pred1, y_true2, y_pred2):\n",
    "    ba1 = balanced_accuracy_score(y_true1, y_pred1)\n",
    "    ba2 = balanced_accuracy_score(y_true2, y_pred2)\n",
    "    return ba2 - ba1\n",
    "\n",
    "\n",
    "def compare_models(df1, df2, eval_regime=None, eval_type=None, n_samples=9999):\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    if eval_regime:\n",
    "        df1 = df1[df1[\"eval_regime\"] == eval_regime]\n",
    "        df2 = df2[df2[\"eval_regime\"] == eval_regime]\n",
    "    if eval_type:\n",
    "        df1 = df1[df1[\"eval_type\"] == eval_type]\n",
    "        df2 = df2[df2[\"eval_type\"] == eval_type]\n",
    "\n",
    "    # Prepare the data from df1\n",
    "    labels1 = df1[\"binary_label\"].values\n",
    "    predictions1 = df1[\"binary_prediction\"].values\n",
    "\n",
    "    # Prepare the data from df2\n",
    "    labels2 = df2[\"binary_label\"].values\n",
    "    predictions2 = df2[\"binary_prediction\"].values\n",
    "\n",
    "    data = (labels1, predictions1, labels2, predictions2)\n",
    "\n",
    "    # Perform bootstrap resampling to compute the difference in balanced accuracy\n",
    "    res = bootstrap(\n",
    "        data,\n",
    "        balanced_accuracy_diff,\n",
    "        paired=True,\n",
    "        confidence_level=0.95,\n",
    "        n_resamples=n_samples,\n",
    "        random_state=42,\n",
    "        alternative=\"greater\",\n",
    "        method=\"basic\",\n",
    "    )\n",
    "\n",
    "    # Extract the results\n",
    "    # diff_observed = balanced_accuracy_diff(labels1, predictions1, labels2, predictions2)\n",
    "    bootstrap_diff_distribution = res.bootstrap_distribution\n",
    "    # confidence_interval = res.confidence_interval\n",
    "    # standard_error = res.standard_error\n",
    "\n",
    "    # Compute p-value for the difference\n",
    "    p_value = np.mean(bootstrap_diff_distribution <= 0)\n",
    "\n",
    "    # Print the results\n",
    "    # print(f\"Observed Difference in Balanced Accuracy: {diff_observed}\")\n",
    "    # print(f\"Bootstrap Confidence Interval: {confidence_interval}\")\n",
    "    # print(f\"Bootstrap Standard Error: {standard_error}\")\n",
    "    # print(f\"Bootstrap p-value: {p_value}\")\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"MAG\": \"MAG\",\n",
    "    # \"MAGBase\": \"MAGBase\",\n",
    "    # \"MAGFreeze\": \"MAGFreeze\",\n",
    "    # \"MAGRace\": \"MAGRace\",\n",
    "    \"MAGWords\": \"MAGWords\",\n",
    "    \"MAGEyes\": \"MAGEyes\",\n",
    "    # \"PostFusion\": \"PostFusion\",\n",
    "    # \"PostFusionFreeze\": \"PostFusionFreeze\",\n",
    "    # \"Baseline - Always A\": \"Roberta\",\n",
    "    \"Baseline - RoBERTaNoEyes\": \"Roberta\",\n",
    "    \"RobertaSelectedAnswersMultiClass\": \"RobertaSelectedAnswersMultiClass\",\n",
    "    \"RoberteyeWord\": \"RoberteyeWord\",\n",
    "    \"RoberteyeWordEyes\": \"RoberteyeWordEyes\",\n",
    "    # \"RoBERTeyeFixation\": \"RoberteyeFixation\",\n",
    "    # \"BEyeLSTM\": \"BEyeLSTMArgs\",\n",
    "    # \"AhnCNN\": \"AhnCNN\",\n",
    "    # \"LR\": \"LR\",\n",
    "    # \"KNN\": \"KNN\",\n",
    "    # \"SVM\": \"SVM\",\n",
    "    # \"Eyettention\": \"Eyettention\",\n",
    "    # \"PostFusionAnswers\": \"PostFusionAnswers\",\n",
    "    # \"PostFusionMultiClass\": \"PostFusionMultiClass\",\n",
    "    # \"PostFusionAnswersMultiClass\": \"PostFusionAnswersMultiClass\",\n",
    "    \"PostFusionSelectedAnswersMultiClass\": \"PostFusionSelectedAnswersMultiClass\",\n",
    "    \"RoberteyeWordSelectedAnswersMultiClass\": \"RoberteyeWordSelectedAnswersMultiClass\",\n",
    "    # \"RoberteyeWordLingSelectedAnswersMultiClass\": \"RoberteyeWordLingSelectedAnswersMultiClass\",\n",
    "    \"RoberteyeFixationSelectedAnswersMultiClass\": \"RoberteyeFixationSelectedAnswersMultiClass\",\n",
    "    \"MAGSelectedAnswersMultiClass\": \"MAGSelectedAnswersMultiClass\",\n",
    "    # \"MAGSelectedAnswersMultiClassLing\": \"MAGSelectedAnswersMultiClassLing\",\n",
    "    \"RoberteyeWordLing\": \"RoberteyeWordLing\",\n",
    "}\n",
    "\n",
    "all_res = defaultdict(dict)\n",
    "for model, model_name in models.items():\n",
    "    trainer = \"IsCorrectSampling\"\n",
    "    if model == \"BEyeLSTM\":\n",
    "        trainer = \"BEyeLSTM\"\n",
    "    if model == \"AhnCNN\":\n",
    "        trainer = \"Ahn\"\n",
    "    if model in [\"LR\", \"KNN\", \"SVM\"]:\n",
    "        trainer = \"ML\"\n",
    "    if model in [\"Eyettention\"]:\n",
    "        trainer = \"Eyettention\"\n",
    "\n",
    "    for data_ in [\"Hunting\", \"Gathering\"]:\n",
    "        res = read_res(\n",
    "            data=data_,\n",
    "            model=model_name,\n",
    "            trainer=trainer,\n",
    "            data_path=\"may05\",\n",
    "            wandb_job_type=f\"hyperparameter_sweep_{model_name}\",\n",
    "            base_res_path=\"emnlp24_results\",\n",
    "            on_error=\"raise\",\n",
    "            base_path=Path(\"..\"),\n",
    "        )\n",
    "        all_res[data_][model] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_compare_modes = [\n",
    "#     # (\"Baseline - RoBERTaNoEyes\", \"RoberteyeWord\"),\n",
    "#     # (\"Baseline - RoBERTaNoEyes\", \"RoBERTeyeFixation\"),\n",
    "#     # (\"Baseline - RoBERTaNoEyes\", \"PostFusion\"),\n",
    "#     # (\"Baseline - RoBERTaNoEyes\", \"BEyeLSTM\"),\n",
    "#     # (\"Baseline - RoBERTaNoEyes\", \"AhnCNN\"),\n",
    "#     # (\"Baseline - RoBERTaNoEyes\", \"LR\"),\n",
    "#     # (\"Baseline - RoBERTaNoEyes\", \"Eyettention\"),\n",
    "#     # (\"Baseline - RoBERTaNoEyes\", \"MAG\"),\n",
    "#     # (\"MAG\", \"MAGFreeze\"),\n",
    "#     # (\"MAG\", \"MAGRace\"),\n",
    "#     # (\"MAG\", \"MAGBase\"),\n",
    "#     # (\"PostFusion\", \"PostFusionFreeze\"),\n",
    "# ]\n",
    "# type_ = \"test\"\n",
    "# for model1, model2 in to_compare_modes:\n",
    "#     for eval_regime in [\"new_item\", \"new_subject\", \"new_item_and_subject\", \"all\"]:\n",
    "#         query = (\n",
    "#             f\"eval_regime == '{eval_regime}'\"\n",
    "#             if eval_regime != \"all\"\n",
    "#             else \"index == index or index != index\"\n",
    "#         )\n",
    "#         g_m1 = all_res[\"Gathering\"][model1].query(query)\n",
    "#         g_m2 = all_res[\"Gathering\"][model2].query(query)\n",
    "#         h_m1 = all_res[\"Hunting\"][model1].query(query)\n",
    "#         h_m2 = all_res[\"Hunting\"][model2].query(query)\n",
    "\n",
    "#         def balanced_accuracies_per_fold(df):\n",
    "#             return [\n",
    "#                 balanced_accuracy_score(\n",
    "#                     df[df[\"fold_index\"] == i][\"binary_label\"],\n",
    "#                     df[df[\"fold_index\"] == i][\"binary_prediction\"],\n",
    "#                 )\n",
    "#                 for i in range(10)\n",
    "#             ]\n",
    "\n",
    "#         # Compute balanced accuracy for each for each model\n",
    "#         acc_g_m1 = balanced_accuracies_per_fold(g_m1)\n",
    "#         acc_g_m2 = balanced_accuracies_per_fold(g_m2)\n",
    "#         acc_h_m1 = balanced_accuracies_per_fold(h_m1)\n",
    "#         acc_h_m2 = balanced_accuracies_per_fold(h_m2)\n",
    "\n",
    "#         assert (\n",
    "#             len(acc_g_m1) == len(acc_g_m2) == len(acc_h_m1) == len(acc_h_m2)\n",
    "#             and len(acc_g_m1) > 0\n",
    "#         )\n",
    "#         # Perform Wilcoxon signed-rank test for each h/g and model pair\n",
    "#         wilcoxon_g = wilcoxon(\n",
    "#             [acc_g_m1[i] - acc_g_m2[i] for i in range(len(acc_g_m2))],\n",
    "#             alternative=\"greater\",\n",
    "#         )\n",
    "#         wilcoxon_h = wilcoxon(\n",
    "#             [acc_h_m1[i] - acc_h_m2[i] for i in range(len(acc_h_m2))],\n",
    "#             alternative=\"greater\",\n",
    "#         )\n",
    "\n",
    "#         print(\n",
    "#             f\"{model1} vs {model2} - {eval_regime} - Gathering: {wilcoxon_g['p-val'].values[0]} - Hunting: {wilcoxon_h['p-val'].values[0]}\"\n",
    "#         )\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare_modes = [\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"RoberteyeWord\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"RoBERTeyeFixation\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"PostFusion\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"BEyeLSTM\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"AhnCNN\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"LR\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"Eyettention\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"MAG\"),\n",
    "]\n",
    "n_samples = 10000\n",
    "for model1, model2 in to_compare_modes:\n",
    "    for eval_regime in [\"new_item\", \"new_subject\", \"new_item_and_subject\", None]:\n",
    "        p_value_g = compare_models(\n",
    "            all_res[\"Gathering\"][model1],\n",
    "            all_res[\"Gathering\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        p_value_h = compare_models(\n",
    "            all_res[\"Hunting\"][model1],\n",
    "            all_res[\"Hunting\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        print(\n",
    "            f\"{model1} vs {model2} - {eval_regime} - Gathering: {p_value_g} - Hunting: {p_value_h}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordSelectedAnswersMultiClass - new_item - Gathering: 0.0357 - Hunting: 0.0002\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordSelectedAnswersMultiClass - new_subject - Gathering: 0.5706 - Hunting: 0.9593\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordSelectedAnswersMultiClass - new_item_and_subject - Gathering: 0.0079 - Hunting: 0.1931\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordSelectedAnswersMultiClass - None - Gathering: 0.0721 - Hunting: 0.1358\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeFixationSelectedAnswersMultiClass - new_item - Gathering: 0.0283 - Hunting: 0.0\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeFixationSelectedAnswersMultiClass - new_subject - Gathering: 0.5237 - Hunting: 0.4157\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeFixationSelectedAnswersMultiClass - new_item_and_subject - Gathering: 0.5379 - Hunting: 0.2019\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeFixationSelectedAnswersMultiClass - None - Gathering: 0.1306 - Hunting: 0.0002\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_item - Gathering: 0.0009 - Hunting: 0.1509\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_subject - Gathering: 0.4282 - Hunting: 0.8401\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_item_and_subject - Gathering: 0.0007 - Hunting: 0.6726\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - None - Gathering: 0.0028 - Hunting: 0.567\n",
      "RobertaSelectedAnswersMultiClass vs PostFusionSelectedAnswersMultiClass - new_item - Gathering: 0.007 - Hunting: 0.0331\n",
      "RobertaSelectedAnswersMultiClass vs PostFusionSelectedAnswersMultiClass - new_subject - Gathering: 0.6779 - Hunting: 0.9977\n",
      "RobertaSelectedAnswersMultiClass vs PostFusionSelectedAnswersMultiClass - new_item_and_subject - Gathering: 0.0381 - Hunting: 0.6172\n",
      "RobertaSelectedAnswersMultiClass vs PostFusionSelectedAnswersMultiClass - None - Gathering: 0.0477 - Hunting: 0.8769\n"
     ]
    }
   ],
   "source": [
    "to_compare_modes = [\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"RoberteyeWordSelectedAnswersMultiClass\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"RoberteyeFixationSelectedAnswersMultiClass\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"MAGSelectedAnswersMultiClass\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"PostFusionSelectedAnswersMultiClass\"),\n",
    "]\n",
    "n_samples = 10000\n",
    "for model1, model2 in to_compare_modes:\n",
    "    for eval_regime in [\"new_item\", \"new_subject\", \"new_item_and_subject\", None]:\n",
    "        p_value_g = compare_models(\n",
    "            all_res[\"Gathering\"][model1],\n",
    "            all_res[\"Gathering\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        p_value_h = compare_models(\n",
    "            all_res[\"Hunting\"][model1],\n",
    "            all_res[\"Hunting\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        print(\n",
    "            f\"{model1} vs {model2} - {eval_regime} - Gathering: {p_value_g} - Hunting: {p_value_h}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_compare_modes = [\n",
    "#     (\"Baseline - RoBERTaNoEyes\", \"RoberteyeWord\"),\n",
    "#     (\"Baseline - RoBERTaNoEyes\", \"RoBERTeyeFixation\"),\n",
    "#     (\"Baseline - RoBERTaNoEyes\", \"PostFusion\"),\n",
    "#     (\"Baseline - RoBERTaNoEyes\", \"BEyeLSTM\"),\n",
    "#     (\"Baseline - RoBERTaNoEyes\", \"AhnCNN\"),\n",
    "#     (\"Baseline - RoBERTaNoEyes\", \"LR\"),\n",
    "#     (\"Baseline - RoBERTaNoEyes\", \"Eyettention\"),\n",
    "#     (\"Baseline - RoBERTaNoEyes\", \"MAG\"),\n",
    "# ]\n",
    "# type_ = \"test\"\n",
    "# for model1, model2 in to_compare_modes:\n",
    "#     for eval_regime in [\"new_item\", \"new_subject\", \"new_item_and_subject\", None]:\n",
    "#         g_m1 = all_res[\"Gathering\"][model1]\n",
    "#         g_m2 = all_res[\"Gathering\"][model2]\n",
    "#         h_m1 = all_res[\"Hunting\"][model1]\n",
    "#         h_m2 = all_res[\"Hunting\"][model2]\n",
    "\n",
    "#         def balanced_accuracies_per_fold(df, eval_regime=None):\n",
    "#             df = df.copy()\n",
    "#             if eval_regime:\n",
    "#                 df = df[df[\"eval_regime\"] == eval_regime]\n",
    "#             return [\n",
    "#                 balanced_accuracy_score(\n",
    "#                     df[df[\"fold_index\"] == i][\"binary_label\"],\n",
    "#                     df[df[\"fold_index\"] == i][\"binary_prediction\"],\n",
    "#                 )\n",
    "#                 for i in range(10)\n",
    "#             ]\n",
    "\n",
    "#         # Compute balanced accuracy for each for each model\n",
    "#         acc_g_m1 = balanced_accuracies_per_fold(g_m1, eval_regime=eval_regime)\n",
    "#         acc_g_m2 = balanced_accuracies_per_fold(g_m2, eval_regime=eval_regime)\n",
    "#         acc_h_m1 = balanced_accuracies_per_fold(h_m1, eval_regime=eval_regime)\n",
    "#         acc_h_m2 = balanced_accuracies_per_fold(h_m2, eval_regime=eval_regime)\n",
    "\n",
    "#         # Perform Wilcoxon signed-rank test for each h/g and model pair\n",
    "#         wilcoxon_g = wilcoxon(acc_g_m1, acc_g_m2, alternative=\"less\")\n",
    "#         wilcoxon_h = wilcoxon(acc_h_m1, acc_h_m2, alternative=\"less\")\n",
    "\n",
    "#         print(\n",
    "#             f\"{model1} vs {model2} - {eval_regime} - Gathering: {round(wilcoxon_g['p-val'].values[0],3)} - Hunting: {round(wilcoxon_h['p-val'].values[0],3)}\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAG vs MAGFreeze - new_item - Gathering: 0.7265 - Hunting: 0.7048\n",
      "MAG vs MAGFreeze - new_subject - Gathering: 1.0 - Hunting: 0.9961\n",
      "MAG vs MAGFreeze - new_item_and_subject - Gathering: 0.8092 - Hunting: 0.275\n",
      "MAG vs MAGFreeze - None - Gathering: 0.998 - Hunting: 0.9703\n",
      "MAG vs MAGRace - new_item - Gathering: 0.5481 - Hunting: 1.0\n",
      "MAG vs MAGRace - new_subject - Gathering: 0.1679 - Hunting: 0.2875\n",
      "MAG vs MAGRace - new_item_and_subject - Gathering: 0.6826 - Hunting: 0.9876\n",
      "MAG vs MAGRace - None - Gathering: 0.3809 - Hunting: 0.9999\n",
      "MAG vs MAGBase - new_item - Gathering: 0.9941 - Hunting: 0.9634\n",
      "MAG vs MAGBase - new_subject - Gathering: 0.5377 - Hunting: 0.0336\n",
      "MAG vs MAGBase - new_item_and_subject - Gathering: 0.1076 - Hunting: 0.4772\n",
      "MAG vs MAGBase - None - Gathering: 0.9593 - Hunting: 0.6329\n",
      "PostFusion vs PostFusionFreeze - new_item - Gathering: 0.1493 - Hunting: 0.9846\n",
      "PostFusion vs PostFusionFreeze - new_subject - Gathering: 0.9994 - Hunting: 0.9997\n",
      "PostFusion vs PostFusionFreeze - new_item_and_subject - Gathering: 0.4848 - Hunting: 0.4616\n",
      "PostFusion vs PostFusionFreeze - None - Gathering: 0.9174 - Hunting: 1.0\n"
     ]
    }
   ],
   "source": [
    "to_compare_modes = [\n",
    "    (\"MAG\", \"MAGFreeze\"),\n",
    "    (\"MAG\", \"MAGRace\"),\n",
    "    (\"MAG\", \"MAGBase\"),\n",
    "    (\"PostFusion\", \"PostFusionFreeze\"),\n",
    "]\n",
    "n_samples = 10000\n",
    "for model1, model2 in to_compare_modes:\n",
    "    for eval_regime in [\"new_item\", \"new_subject\", \"new_item_and_subject\", None]:\n",
    "        p_value_g = compare_models(\n",
    "            all_res[\"Gathering\"][model1],\n",
    "            all_res[\"Gathering\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        p_value_h = compare_models(\n",
    "            all_res[\"Hunting\"][model1],\n",
    "            all_res[\"Hunting\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        print(\n",
    "            f\"{model1} vs {model2} - {eval_regime} - Gathering: {p_value_g} - Hunting: {p_value_h}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline - RoBERTaNoEyes vs RoberteyeWord - new_item - Gathering: 0.1628 - Hunting: 0.9133\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWord - new_subject - Gathering: 0.2574 - Hunting: 0.144\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWord - new_item_and_subject - Gathering: 0.9349 - Hunting: 0.4356\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWord - None - Gathering: 0.2164 - Hunting: 0.6512\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWordLing - new_item - Gathering: 0.0054 - Hunting: 0.9911\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWordLing - new_subject - Gathering: 0.1646 - Hunting: 0.3996\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWordLing - new_item_and_subject - Gathering: 0.1497 - Hunting: 0.4226\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWordLing - None - Gathering: 0.0024 - Hunting: 0.9605\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWordEyes - new_item - Gathering: 0.1953 - Hunting: 0.781\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWordEyes - new_subject - Gathering: 0.3578 - Hunting: 0.6706\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWordEyes - new_item_and_subject - Gathering: 0.3138 - Hunting: 0.4722\n",
      "Baseline - RoBERTaNoEyes vs RoberteyeWordEyes - None - Gathering: 0.1591 - Hunting: 0.7949\n",
      "Baseline - RoBERTaNoEyes vs MAG - new_item - Gathering: 0.4563 - Hunting: 0.2307\n",
      "Baseline - RoBERTaNoEyes vs MAG - new_subject - Gathering: 0.0427 - Hunting: 0.8779\n",
      "Baseline - RoBERTaNoEyes vs MAG - new_item_and_subject - Gathering: 0.7452 - Hunting: 0.3705\n",
      "Baseline - RoBERTaNoEyes vs MAG - None - Gathering: 0.1679 - Hunting: 0.5128\n",
      "Baseline - RoBERTaNoEyes vs MAGWords - new_item - Gathering: 0.7742 - Hunting: 0.4628\n",
      "Baseline - RoBERTaNoEyes vs MAGWords - new_subject - Gathering: 0.165 - Hunting: 0.3309\n",
      "Baseline - RoBERTaNoEyes vs MAGWords - new_item_and_subject - Gathering: 0.2294 - Hunting: 0.1301\n",
      "Baseline - RoBERTaNoEyes vs MAGWords - None - Gathering: 0.4011 - Hunting: 0.2732\n",
      "Baseline - RoBERTaNoEyes vs MAGEyes - new_item - Gathering: 0.0384 - Hunting: 0.3069\n",
      "Baseline - RoBERTaNoEyes vs MAGEyes - new_subject - Gathering: 0.0946 - Hunting: 0.3327\n",
      "Baseline - RoBERTaNoEyes vs MAGEyes - new_item_and_subject - Gathering: 0.4388 - Hunting: 0.0704\n",
      "Baseline - RoBERTaNoEyes vs MAGEyes - None - Gathering: 0.0153 - Hunting: 0.154\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_item - Gathering: 0.0009 - Hunting: 0.1509\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_subject - Gathering: 0.4282 - Hunting: 0.8401\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_item_and_subject - Gathering: 0.0007 - Hunting: 0.6726\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - None - Gathering: 0.0028 - Hunting: 0.567\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MAGSelectedAnswersMultiClassLing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model1, model2 \u001b[38;5;129;01min\u001b[39;00m to_compare_modes:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_regime \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_item\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_subject\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_item_and_subject\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m     20\u001b[0m         p_value_g \u001b[38;5;241m=\u001b[39m compare_models(\n\u001b[1;32m     21\u001b[0m             all_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGathering\u001b[39m\u001b[38;5;124m\"\u001b[39m][model1],\n\u001b[0;32m---> 22\u001b[0m             \u001b[43mall_res\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGathering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     23\u001b[0m             eval_regime\u001b[38;5;241m=\u001b[39meval_regime,\n\u001b[1;32m     24\u001b[0m             eval_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m             n_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[1;32m     26\u001b[0m         )\n\u001b[1;32m     27\u001b[0m         p_value_h \u001b[38;5;241m=\u001b[39m compare_models(\n\u001b[1;32m     28\u001b[0m             all_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHunting\u001b[39m\u001b[38;5;124m\"\u001b[39m][model1],\n\u001b[1;32m     29\u001b[0m             all_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHunting\u001b[39m\u001b[38;5;124m\"\u001b[39m][model2],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m             n_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[1;32m     33\u001b[0m         )\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_regime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Gathering: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value_g\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Hunting: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value_h\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m         )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MAGSelectedAnswersMultiClassLing'"
     ]
    }
   ],
   "source": [
    "to_compare_modes = [\n",
    "    # ----------------------------------------------\n",
    "    # Table 7: Input ablation - binary\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"RoberteyeWord\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"RoberteyeWordLing\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"RoberteyeWordEyes\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"MAG\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"MAGWords\"),\n",
    "    (\"Baseline - RoBERTaNoEyes\", \"MAGEyes\"),\n",
    "    # ----------------------------------------------\n",
    "    # # Table 8: Input ablation - multiclass\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"MAGSelectedAnswersMultiClass\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"MAGSelectedAnswersMultiClassLing\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"RoberteyeWordSelectedAnswersMultiClass\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"RoberteyeWordLingSelectedAnswersMultiClass\"),\n",
    "]\n",
    "n_samples = 10000\n",
    "for model1, model2 in to_compare_modes:\n",
    "    for eval_regime in [\"new_item\", \"new_subject\", \"new_item_and_subject\", None]:\n",
    "        p_value_g = compare_models(\n",
    "            all_res[\"Gathering\"][model1],\n",
    "            all_res[\"Gathering\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        p_value_h = compare_models(\n",
    "            all_res[\"Hunting\"][model1],\n",
    "            all_res[\"Hunting\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        print(\n",
    "            f\"{model1} vs {model2} - {eval_regime} - Gathering: {p_value_g} - Hunting: {p_value_h}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_item - Gathering: 0.0009 - Hunting: 0.1509\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_subject - Gathering: 0.4282 - Hunting: 0.8401\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - new_item_and_subject - Gathering: 0.0007 - Hunting: 0.6726\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClass - None - Gathering: 0.0028 - Hunting: 0.567\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClassLing - new_item - Gathering: 0.0145 - Hunting: 0.0084\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClassLing - new_subject - Gathering: 0.6887 - Hunting: 0.8197\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClassLing - new_item_and_subject - Gathering: 0.0626 - Hunting: 0.0149\n",
      "RobertaSelectedAnswersMultiClass vs MAGSelectedAnswersMultiClassLing - None - Gathering: 0.0743 - Hunting: 0.0952\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordSelectedAnswersMultiClass - new_item - Gathering: 0.0357 - Hunting: 0.0002\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordSelectedAnswersMultiClass - new_subject - Gathering: 0.5706 - Hunting: 0.9593\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordSelectedAnswersMultiClass - new_item_and_subject - Gathering: 0.0079 - Hunting: 0.1931\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordSelectedAnswersMultiClass - None - Gathering: 0.0721 - Hunting: 0.1358\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordLingSelectedAnswersMultiClass - new_item - Gathering: 0.0001 - Hunting: 0.0049\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordLingSelectedAnswersMultiClass - new_subject - Gathering: 0.7926 - Hunting: 0.7516\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordLingSelectedAnswersMultiClass - new_item_and_subject - Gathering: 0.0135 - Hunting: 0.5735\n",
      "RobertaSelectedAnswersMultiClass vs RoberteyeWordLingSelectedAnswersMultiClass - None - Gathering: 0.0086 - Hunting: 0.1373\n"
     ]
    }
   ],
   "source": [
    "to_compare_modes = [\n",
    "    # ----------------------------------------------\n",
    "    # Table 8: Input ablation - multiclass\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"MAGSelectedAnswersMultiClass\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"MAGSelectedAnswersMultiClassLing\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"RoberteyeWordSelectedAnswersMultiClass\"),\n",
    "    (\"RobertaSelectedAnswersMultiClass\", \"RoberteyeWordLingSelectedAnswersMultiClass\"),\n",
    "]\n",
    "n_samples = 10000\n",
    "for model1, model2 in to_compare_modes:\n",
    "    for eval_regime in [\"new_item\", \"new_subject\", \"new_item_and_subject\", None]:\n",
    "        p_value_g = compare_models(\n",
    "            all_res[\"Gathering\"][model1],\n",
    "            all_res[\"Gathering\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        p_value_h = compare_models(\n",
    "            all_res[\"Hunting\"][model1],\n",
    "            all_res[\"Hunting\"][model2],\n",
    "            eval_regime=eval_regime,\n",
    "            eval_type=\"test\",\n",
    "            n_samples=n_samples,\n",
    "        )\n",
    "        print(\n",
    "            f\"{model1} vs {model2} - {eval_regime} - Gathering: {p_value_g} - Hunting: {p_value_h}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
